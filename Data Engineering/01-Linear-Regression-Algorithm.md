<h1> ðŸ‘½ Linear Regression Algorithm </h1>

<p>In a few words, linear regression is a statistical method for modeling relationships between a dependent variable with a given set of independent variables. It is okay if you did not understand a damn thing - at first, neither me - but we gonna dissecate that terms before see the code.</p>

<p> **Independent Variables:** is the cause. Its value is independent of other variables in your study. </p>

> Is the temperature of the room. You vary the room temperature by making it cooler for half the participants, and warmer for the other half.

<p> **Dependent Variables:** is the effect. Its value depends on changes in the independent variable.</p>

> Is math test scores. You measure the math skills of all participants using a standardized test and check whether they differ based on room temperature.

<p> So, that code represents the relationship between a extrinsic parameter of the human factor (independent variable) and another intrinsic parameter of the human factor (dependent variable).</p>

<h2>ðŸ›¸ Simple Linear Regression </h2>
<p>Is an approach for predicting a response using a single feature. It is assumed that the two variables are **linearly related**. Hence, we try to find a linear function that predicts the response value(y) as accurately as possible as a function of the feature or independent variable(x). Let us consider a dataset where we have a value of response ***y*** for every feature ***x***:</p>

>A linear relationship is one where increasing or decreasing one variable ***n*** times will cause a corresponding increase or decrease of ***n*** times in the other variable too. In simpler words, if you double one variable, the other will double as well.

Dataset | v1 | v2 | v3 | v4 | v5 | v6 | v7 | v8 | v9 | v10
|:-----:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
***x*** | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 |
***y*** | 1 | 3 | 2 | 5 | 7 | 8 | 8 | 9 | 10 | 12 |

<p> For generality, we define:</p>

- ***x*** as feature vector, i.e ***x*** = [x_1, x_2, â€¦., x_n],
- ***y*** as response vector, i.e ***y*** = [y_1, y_2, â€¦., y_n]
- for ***n*** observations (in above example, n=10).

<p> A scatter plot of the above dataset looks like: </p>

![](https://media.geeksforgeeks.org/wp-content/uploads/python-linear-regression-1.png)

<p>Now, the task is to find ***a line that fits best in the above scatter plot*** so that we can predict the response for any new feature values. (i.e a value of ***x*** not present in a dataset). This line is called a ***regression line***. The equation of regression line is represented as: </p>

$$h(x_i)= Î²_0 + Î²_1 â‹… x_i$$

<p>The first part represents the predicted response value for ith observation; the second and third part about Î² are regression coefficients and represent y-intercept and slope of regression line respectively - once weâ€™ve estimated these coefficients, we can use the model to predict responses! Now consider:</p>

$$y_i = Î²_0 + Î²_i â‹… x_i + Îµ_i  = h(x_i) + Îµ_i$$

$$Îµ_i = y_i - h(x_i)$$

<p>Here, Îµ is a residual error in ith observation.  So, our aim is to minimize the total residual error. We define the squared error or cost function, J as: </p>

$$J(Î²_0, Î²_1) = (1/2n) â‹… \sum_{i=1}^n Îµ_i^2$$

<p>The values in the parentheses of J are the determined values.</p>

$$Î²_1 = SS_xy/SS_xx$$

$$Î²_0 = yÌ… -Î² _1 â‹… xÌ…$$

$$SS_xy = \sum_{i=1}^n (x_i - xÌ…) â‹… (y_i - yÌ…) = \sum_{i=1}^n (y_iâ‹…x_i - n â‹… xÌ… â‹… yÌ…)$$

$$SS_xx = \sum_{i=1}^n (x_i - xÌ…)^2 = \sum_{i=1}^n (x_i^2 - n â‹… (xÌ…)^2)$$

<p>And graph obtained looks like this:</p>

![](https://media.geeksforgeeks.org/wp-content/uploads/python-linear-regression-2.png)

<h2>ðŸ›¸ Multiple Linear Regression </h2>
<p>Multiple linear regression attempts to model the relationship between two or more features and a response by fitting a linear equation to the observed data. Clearly, it is nothing but an extension of simple linear regression. Consider a dataset with p features(or independent variables) and one response(or dependent variable). 
Also, the dataset contains n rows/observations. We define: ***x*** (feature matrix) = a matrix of size ***n X p*** where:</p>

$$x_{ij}$$

<p>Denotes the values of jth feature for ith observation. So: </p>

>In this explanation, I refer to dependent variables as responses and independent variables as features for simplicity.

$$\begin{pmatrix}
x_11 & â‹¯ & x_1p\\
x_21 & â‹¯ & x_2p\\
â‹® & â‹± & â‹®\\
x_n1 & â‹¯ & x_np\
\end{pmatrix}$$

$$y = Xâ‹… Î² + Îµ$$

$$Î² = \begin{bmatrix}
Î²_0\\
Î²_1\\
â‹®\\
Î²_p
\end{bmatrix},  Îµ = \begin{bmatrix}
Îµ_1\\
Îµ_2\\
â‹®\\
Îµ_n
\end{bmatrix}$$

<p>Now, we determine an estimate of b, i.e. bâ€™ using the Least Squares method. As already explained, the Least Squares method tends to determine bâ€™ for which total residual error is minimized. We present the result directly here:  </p>
